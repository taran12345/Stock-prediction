{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv(\"stock.txt\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>OpenInt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-01-02</td>\n",
       "      <td>0.6277</td>\n",
       "      <td>0.6362</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>2575579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-01-03</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>1764749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-01-04</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>2194010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-01-05</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.5798</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>3255244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-01-08</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>3696430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1962-01-09</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>2778285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1962-01-10</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>2337096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1962-01-11</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>1943605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1962-01-12</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>2015151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1962-01-15</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>2527879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close   Volume  OpenInt\n",
       "0  1962-01-02  0.6277  0.6362  0.6201  0.6201  2575579        0\n",
       "1  1962-01-03  0.6201  0.6201  0.6122  0.6201  1764749        0\n",
       "2  1962-01-04  0.6201  0.6201  0.6037  0.6122  2194010        0\n",
       "3  1962-01-05  0.6122  0.6122  0.5798  0.5957  3255244        0\n",
       "4  1962-01-08  0.5957  0.5957  0.5716  0.5957  3696430        0\n",
       "5  1962-01-09  0.5957  0.6037  0.5878  0.5957  2778285        0\n",
       "6  1962-01-10  0.5957  0.6037  0.5957  0.5957  2337096        0\n",
       "7  1962-01-11  0.5957  0.5957  0.5878  0.5957  1943605        0\n",
       "8  1962-01-12  0.5957  0.6037  0.5878  0.5878  2015151        0\n",
       "9  1962-01-15  0.5957  0.5957  0.5957  0.5957  2527879        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.drop(\"Date\", axis=1, inplace=True)\n",
    "stock.drop(\"OpenInt\", axis=1, inplace=True)\n",
    "stock.drop(\"Volume\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6277</td>\n",
       "      <td>0.6362</td>\n",
       "      <td>0.6201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.5798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.5716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open    High     Low\n",
       "0  0.6277  0.6362  0.6201\n",
       "1  0.6201  0.6201  0.6122\n",
       "2  0.6201  0.6201  0.6037\n",
       "3  0.6122  0.6122  0.5798\n",
       "4  0.5957  0.5957  0.5716"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = stock.drop(\"Close\", axis=1)\n",
    "Y = stock[\"Close\"] \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14058, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14058, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14058,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14058, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we take reshape of y bz without it y.shape gives (14058,)\n",
    "Y = np.array(Y) \n",
    "Y=Y.reshape(-1,1)\n",
    "Y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a,n_x,n_y):\n",
    "    Wf = np.random.randn(n_a, n_a+n_x)* np.sqrt(2/n_a+n_x)\n",
    "    bf = np.random.randn(n_a,1)\n",
    "    Wi = np.random.randn(n_a, n_a+n_x)* np.sqrt(2/n_a+n_x)\n",
    "    bi = np.random.randn(n_a,1)\n",
    "    Wo = np.random.randn(n_a, n_a+n_x)* np.sqrt(2/n_a+n_x)\n",
    "    bo = np.random.randn(n_a,1)\n",
    "    Wc = np.random.randn(n_a, n_a+n_x)* np.sqrt(2/n_a+n_x)\n",
    "    bc = np.random.randn(n_a,1)\n",
    "    Wy = np.random.randn(n_y,n_a)* np.sqrt(2/n_a)\n",
    "    by = np.random.randn(n_y,1)\n",
    "    \n",
    "#   parameters -- python dictionary containing weights and bias\n",
    "    parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start by implementing the LSTM cell for a single time-step. Then you can iteratively call \n",
    "# it from inside a for-loop tohave it process an input with T_x time-steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \n",
    "#   xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "#   a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "#   c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    n_x,m = xt.shape\n",
    "    n_a,_ = a_prev.shape\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Concatenate a_prev and xt (≈3 lines)\n",
    "    concat = np.zeros((n_a + n_x, m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt\n",
    "    \n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)   \n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "# * means element wise multiplication if 4*3 matrix multiply by 4*3 then resultant matrix also be of 4*3\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "#  ft,it,cct,c_next,ot,a_next will be of na,m \n",
    "\n",
    "    yt_pred = np.dot(Wy, a_next) + by\n",
    "#   yt will be of ny,nm     \n",
    "       \n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (ft,it,ot,cct,c_next,a_next,yt_pred,xt,a_prev,c_prev,parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(X, a0, parameters):\n",
    "# x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "# a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    \n",
    "# Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    n_y,n_a = parameters[\"Wy\"].shape\n",
    "    n_x, m, T_x = X.shape \n",
    "            \n",
    "# Initialize a_next and c_next \n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)\n",
    "    \n",
    "# initialize \"a\", \"c\" and \"y\" with zeros \n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "\n",
    "        \n",
    "# loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(X[:, :, t], a_next, c_next, parameters)\n",
    "        \n",
    "        a[:,:,t] = a_next\n",
    "    \n",
    "        c[:,:,t]  = c_next\n",
    "        \n",
    "        y[:,:,t] = yt\n",
    "        \n",
    "        caches.append(cache)\n",
    "     \n",
    "    \n",
    "# store values needed for backward propagation in cache\n",
    "    caches = (caches, X)\n",
    "\n",
    "    return y,a,c,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, Y):\n",
    "    _,m,_ = Y.shape\n",
    "    \n",
    "    cost = np.sqrt((1/m)*(np.sum((y-Y)**2)))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, Y, caches):\n",
    "# da_next -- Gradient of loss with respect to next hidden state\n",
    "# cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "# Retrieve values from cache\n",
    "    (ft,it,ot,cct,c_next,a_next,yt_pred,xt,a_prev,c_prev,parameters) = caches\n",
    "    \n",
    "# Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_prev.shape\n",
    "    \n",
    "# calculating da_next and dc_next used for updating gates\n",
    "    dy = Y-yt_pred\n",
    "    da_next = np.dot(parameters['Wy'].T, dy) + da_next\n",
    "    dc_next = (da_next * ot * (1-np.tanh(c_next**2))) + dc_next\n",
    "       \n",
    "    \n",
    "# updating gates which is used for weights and bias\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * it * (1 - cct ** 2)\n",
    "    dit = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * cct * (1 - it) * it\n",
    "    dft = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * c_prev * ft * (1 - ft)   \n",
    "\n",
    "\n",
    "    concat = np.zeros((n_a+n_x,m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a:,:] = xt\n",
    "    \n",
    "# updating weights and bias \n",
    "    dWf = np.dot(dft, concat.T)\n",
    "    dWi = np.dot(dit, concat.T)\n",
    "    dWo = np.dot(dot, concat.T)\n",
    "    dWc = np.dot(dcct, concat.T)\n",
    "    dbf = np.sum(dft, axis = 1, keepdims=True)\n",
    "    dbi = np.sum(dit, axis = 1, keepdims=True)\n",
    "    dbo = np.sum(dot, axis = 1, keepdims=True)\n",
    "    dbc = np.sum(dcct, axis = 1, keepdims=True)\n",
    "    \n",
    "    dWy = np.dot(dy, a_next.T)\n",
    "    dby = np.sum(dy, axis = 1, keepdims = True)\n",
    "    \n",
    "#     da_prev = np.dot(Wf[:, :n_a].T, df_gate) + np.dot(Wc[:, :n_a].T, dc_tilda) + np.dot(Wu[:, :n_a].T, du_gate) + np.dot(Wo[:, :n_a].T, do_gate)\n",
    "#     dc_prev = (da_next * o_gate * (1 - np.tanh(c_next) ** 2) + dc_next) * f_gate\n",
    "#     dxt = np.dot(Wf[:, n_a:].T, df_gate) + np.dot(Wc[:, n_a:].T, dc_tilda) + np.dot(Wu[:, n_a:].T, du_gate) + np.dot(Wo[:, n_a:].T, do_gate)\n",
    "    \n",
    "    dAX = np.dot(parameters[\"Wf\"].T, dft) + np.dot(parameters['Wi'].T, dit) + np.dot(parameters['Wo'].T, dot) + np.dot(parameters['Wc'].T, dcct)\n",
    "     # dAX = derivative of both da_prev and dx calculated combined\n",
    "    \n",
    "    da_prev = dAX[:n_a,:]\n",
    "    dxt = dAX[n_a:,:]\n",
    "    \n",
    "    dc_prev = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * ft\n",
    "\n",
    "\n",
    "    \n",
    "    gradients = {\n",
    "        'da_prev': da_prev, 'dxt':dxt, 'dc_prev':dc_prev, 'dWf':dWf, 'dbf':dbf, 'dWi':dWi,'dbi':dbi, \"dWo\":dWo,'dbo':dbo, 'dWc':dWc, 'dbc':dbc, \"dWy\":dWy,'dby':dby\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "# Returns:\n",
    "# gradients -- python dictionary containing:\n",
    "#              dx -- Gradients of input data, of shape (n_x, m)\n",
    "#              da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "#              dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "#              dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "#              dba -- Gradients of bias vector, of shape (n_a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(Y, caches):\n",
    "    (caches, X) = caches\n",
    "    (ft, it, ot, cct, c_next, a_next, yt_pred, xt, a_prev, c_prev, parameters) = caches[0]\n",
    "   \n",
    "    n_x, m = xt.shape\n",
    "    n_y, m = yt_pred.shape \n",
    "    n_a,m = a_prev.shape\n",
    "    T_x = len(caches)\n",
    "    \n",
    "# initialize the gradients with the right sizes \n",
    "    dx = np.zeros((n_x, m, T_x))  \n",
    "# dx is not needed here it is used in the LSTM model where output of one is input for other timestep\n",
    "    da0 = np.zeros((n_a, m))\n",
    "# da0 is updated in backpropgation beacuse intially we define a0 with a matrix of zero but we will use this da0 matrix in forward propogation for \n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWi = np.zeros((n_a, n_a + n_x))\n",
    "    dWc = np.zeros((n_a, n_a + n_x))\n",
    "    dWo = np.zeros((n_a, n_a + n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))\n",
    "    \n",
    " # loop back over the whole sequence\n",
    "    for t in reversed(range(T_x)):\n",
    "        gradients = lstm_cell_backward(da_prevt, dc_prevt, Y[:,:,t], caches[t])\n",
    "        \n",
    "        dWf = gradients['dWf']\n",
    "        dWo = gradients['dWo']\n",
    "        dWi = gradients['dWi']\n",
    "        dWc = gradients['dWc']\n",
    "        dWy = gradients['dWy']\n",
    "        dbf = gradients['dbf']\n",
    "        dbo = gradients['dbo']\n",
    "        dbi = gradients['dbi']\n",
    "        dbc = gradients['dbc']\n",
    "        dby = gradients['dby']\n",
    "        \n",
    "        da_prevt = gradients['da_prev']\n",
    "        dc_prevt = gradients['dc_prev']\n",
    "    \n",
    "    da0 = da_prevt  \n",
    "#   last da_prevt during backpropogation is set to da0 which will act a0 during forward proppgation bz we set ao as a matrix of zero \n",
    "\n",
    "    gradients = {'da0':da0, 'dWf':dWf, 'dbf':dbf, 'dWo':dWo, 'dbo':dbo, 'dWi':dWi, 'dbi':dbi, \n",
    "                'dWc':dWc, 'dbc':dbc, 'dWy':dWy, 'dby':dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    parameters['Wf'] += learning_rate * gradients['dWf']\n",
    "    parameters['Wo'] += learning_rate * gradients['dWo']\n",
    "    parameters['Wi'] += learning_rate * gradients['dWi']\n",
    "    parameters['Wc'] += learning_rate * gradients['dWc']\n",
    "    parameters['Wy'] += learning_rate * gradients['dWy']\n",
    "    \n",
    "    parameters['bf'] += learning_rate * gradients['dbf']\n",
    "    parameters['bo'] += learning_rate * gradients['dbo']\n",
    "    parameters['bi'] += learning_rate * gradients['dbi']\n",
    "    parameters['bc'] += learning_rate * gradients['dbc']\n",
    "    parameters['by'] += learning_rate * gradients['dby']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, num_iterations=2000, learning_rate = 0.1, print_cost = True):\n",
    "    costs = []\n",
    "    \n",
    "    n_x = X_train.shape[0]\n",
    "    n_y = Y_train.shape[0]\n",
    "    n_a = 10\n",
    "    parameters = initialize_parameters(n_a,n_x,n_y)\n",
    "    a0 = np.random.randn(n_a,1)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        y, a, c,caches = lstm_forward(X, a0, parameters)\n",
    "        \n",
    "        cost = compute_cost(y,Y)\n",
    "        \n",
    "        grads = lstm_backward(Y, caches)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i%100 == 0:\n",
    "            print('Cost after iteration %i: %f' %(i, cost))\n",
    "        if print_cost and i%100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('Iterations (per hundered)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Learning rate = ' +str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1]).T\n",
    "X_test = X_test.values\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1]).T\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1, Y_train.shape[1]).T\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1, Y_test.shape[1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 9840)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 4218)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1503.408225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taran\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 1566.428662\n",
      "Cost after iteration 200: 1566.428662\n",
      "Cost after iteration 300: 1566.428662\n",
      "Cost after iteration 400: 1566.428662\n",
      "Cost after iteration 500: 1566.428662\n",
      "Cost after iteration 600: 1566.428662\n",
      "Cost after iteration 700: 1566.428662\n",
      "Cost after iteration 800: 1566.428662\n",
      "Cost after iteration 900: 1566.428662\n",
      "Cost after iteration 1000: 1566.428662\n",
      "Cost after iteration 1100: 1566.428662\n",
      "Cost after iteration 1200: 1566.428662\n",
      "Cost after iteration 1300: 1566.428662\n",
      "Cost after iteration 1400: 1566.428662\n",
      "Cost after iteration 1500: 1566.428662\n",
      "Cost after iteration 1600: 1566.428662\n",
      "Cost after iteration 1700: 1566.428662\n",
      "Cost after iteration 1800: 1566.428662\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
